# ğŸ¤– Chatbot with RAG (Retrieval-Augmented Generation)

A full-stack AI chatbot powered by **Llama 3.1 (8B-Instruct Q5_K_M)** and a **Retrieval-Augmented Generation (RAG)** pipeline.  
Built with **Express.js** for the backend and **React** for the frontend, this project runs locally and connects directly to a locally-hosted Large Language Model via the Ollama runtime.

---

## ğŸ§  Overview

This chatbot serves as an interactive local AI assistant with retrieval capabilities â€” allowing it to answer queries based not only on its base model knowledge but also on additional contextual data through RAG.  
The system consists of:

- **Express backend** â€” Handles API routes and streams LLM responses.  
- **React frontend** â€” Static chat UI served by the backend.  
- **Ollama model runtime** â€” Hosts the local Llama 3.1 model and executes the actual inference.  

---

## âš™ï¸ Tech Stack

| Layer | Technology |
|:------|:------------|
| **Backend** | Node.js / Express |
| **Frontend** | React + Vite |
| **AI Runtime** | Ollama (Llama 3.1:8b-instruct-q5_K_M) |
| **RAG Component** | Custom vector retrieval and embedding logic |
| **Streaming** | Server-Sent Events (SSE) |

---

## ğŸš€ Features

- ğŸ”„ **RAG endpoint** (`/rag/stream`) for streaming contextual responses  
- âš¡ **Real-time token streaming** to the UI using SSE  
- ğŸ’¾ **Local inference** â€” no external API calls required  
- ğŸ§© **React chat interface** served by the same Express backend  
- ğŸ› ï¸ **Easy local deployment** via Node scripts  
- ğŸ§± **Extendable architecture** for future data sources or models  

---

## ğŸ“ Project Structure


